{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ProMI Exercise Sheet 1: Basic Probabilities & Information Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first exercise sheet is about the basics of probabilities and information theory.\n",
    "The exercises are a mix of theoretical questions and practical coding exercises.\n",
    "\n",
    "For the coding exercises, we will use Python as our main programming language and rely on the NumPy and Matplotlib packages.\n",
    "If you are not familiar with Python, Jupyter Notebooks, NumPy, or Matplotlib, we recommend you go through the following tutorial before starting with the exercises: https://cs231n.github.io/python-numpy-tutorial/.\n",
    "\n",
    "The theoretical questions can be answered by adding your answers in the markdown cells and using LaTeX to write down the mathematical formulas.\n",
    "Wrap your LaTeX formulas with a dollar sign, like this: `$ f(x) = 2 $` or `$$ f(x) = 2 $$` for a formula on a new line.\n",
    "These would show as $ f(x) = 2 $ and $$ f(x) = 2 $$\n",
    "You can double-click on a markdown cell to edit it."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy.stats import geom"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "def test_true(test_name, value: bool):\n",
    "    if value:\n",
    "        #print(\"\\033[92mTest {}: passed.\\033[0m\".format(test_name))\n",
    "        print(f'Test {test_name}: passed.')\n",
    "    else:\n",
    "        #print(\"\\033[91mTest {}: failed.\\033[0m\".format(test_name))\n",
    "        print(f'Test {test_name}: failed.')\n",
    "\n",
    "# equality\n",
    "def test_almost_equal(test_name, value: float, target: float, precision: float = 1e-4):\n",
    "    test_true(test_name, abs(value - target) < precision)\n",
    "\n",
    "# almost equal\n",
    "def test_almost_zero(test_name, value: float, precision: float = 1e-4):\n",
    "    test_almost_equal(test_name, value, 0.0, precision=precision)\n",
    "\n",
    "def test_almost_equal_array(test_name, value: np.ndarray, target: np.ndarray, precision: float = 1e-4):\n",
    "    test_true(test_name, np.all(np.abs(value - target) < precision))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Basic Probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1.1: Bonferroni Inequality\n",
    "\n",
    "Let $A$ and $B$ be two events in a probability space. Prove the following inequality (known as the Bonferroni inequality):\n",
    "$$ P(A, B) \\geq P(A) + P(B) - 1 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### YOUR SOLUTION HERE:\n",
    "$$ P (A \\cup B) = P(A) + P(B) - P(A \\cap B) $$\n",
    "$$ P (A \\cup B) \\geq P(A \\cap B) $$\n",
    "$$ 1 \\geq P (A \\cup B) \\geq P(A \\cap B) $$\n",
    "$$ 1 \\geq P(A) + P(B) - P(A \\cap B) $$\n",
    "$$ P (A \\cap B) \\geq P(A) + P(B) - 1 $$\n",
    "$$ P(A, B) \\geq P(A) + P(B) - 1 $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1.2: Simple Probability Density Functions\n",
    "\n",
    "The PDF of a random variable $X$ is given by:\n",
    "$$f(x) = \n",
    "\\begin{cases} \n",
    "c(2 - 2x^2) & \\text{for } -1 < x < 1 \\\\ \n",
    "0 & \\text{otherwise} \n",
    "\\end{cases}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Find the value of $c$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### YOUR SOLUTION HERE:\n",
    "\n",
    "$$ \\int_{-\\infty}^{\\infty} f(x) \\, dx = 1 $$\n",
    "$$ \\int_{-1}^{1} c (2 - 2x ^ 2) \\, dx = 1 $$\n",
    "$$ = c \\int_{-1}^{1} (2 - 2x ^ 2) \\, dx $$\n",
    "$$ = c \\left(\\int_{-1}^{1} 2 \\, dx - \\int_{-1}^{1} 2x^2 \\, dx \\right) $$\n",
    "$$ = c \\left( \\left[ 2x \\right] _ {-1}^{1} - \\left[ \\frac{2}{3} x^3 \\right] _ {-1}^{1} \\right) $$\n",
    "$$ = c \\left( 2 - \\left( -2 \\right) - \\left( \\frac{2}{3} - \\left( - \\frac{2}{3} \\right) \\right) \\right) $$\n",
    "$$ = c \\left( 4 - \\frac{4}{3} \\right) $$\n",
    "$$ $$\n",
    "$$ c \\left( \\frac{8}{3} \\right) = 1 $$\n",
    "$$ c = \\frac{1}{ \\frac{8}{3} } = \\frac{3}{8} $$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Compute the expected value $E[X]$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### YOUR SOLUTION HERE:\n",
    "\n",
    "$$ E [ X ] = \\int_{x \\in X }{} x f(x) \\, dx $$\n",
    "$$ = \\int_{-1}^{1} x \\frac{3}{8} \\left( 2 - 2x^2 \\right) \\, dx $$\n",
    "$$ = \\frac{3}{8} \\int_{-1}^{1} 2x - 2x^3 \\, dx $$\n",
    "$$ = \\frac{3}{8} \\left( \\int_{-1}^{1} 2x \\, dx - \\int_{-1}^{1} 2x^3 \\, dx \\right)$$\n",
    "$$ = \\frac{3}{8} \\left( \\left[ x^2 \\right] _{-1}^{1} - \\left[ \\frac{1}{2} x^4 \\right] _{-1}^{1} \\right)$$\n",
    "$$ = 0 $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. What is the CDF of $X$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### YOUR SOLUTION HERE:\n",
    "\n",
    "$$ F _ { X } ( x ) = P ( X \\leq x ) = \\int _ { - \\infty } ^ { x } \\rho ( x ^ { \\prime } ) d x ^ { \\prime } $$\n",
    "$$ F _ { X } ( x ) = \\int _ { - 1 } ^ { x } \\frac { 3 } { 8 } ( 2 - 2 t ^ { 2 } ) d t $$\n",
    "$$ = \\frac { 3 } { 8 } \\left( \\int _ { - 1 } ^ { x } d t - \\int _ { - 1 } ^ { x } 2 t ^ { 2 } d t \\right) $$\n",
    "$$ = \\frac { 3 } { 8 } \\left( [ 2 t ] _{-1}^{x} - \\left[ \\frac { 2 } { 3 } t ^ { 3 } \\right] _{-1} ^ {x} \\right) $$\n",
    "$$ = \\frac { 3 } { 8 } \\left( 2 ( x + 1 ) - \\frac { 2 } { 3 } ( x ^ { 3 } + 1 ) \\right) $$\n",
    "$$ = \\frac { 3 } { 4 } x + \\frac { 3 } { 4 } - \\frac { 1 } { 4 } x ^ { 3 } - \\frac { 1 } { 4 } $$\n",
    "$$ F _ { X } ( x ) = - \\frac { 1 } { 4 } x ^ { 3 } + \\frac { 3 } { 4 } x + \\frac { 1 } { 2 } $$\n",
    "\n",
    "$$F_{X}(x) =\n",
    "\\begin{cases}\n",
    "0 & \\text{for } x < -1 \\\\\n",
    "- \\frac { 1 } { 4 } x ^ { 3 } + \\frac { 3 } { 4 } x + \\frac { 1 } { 2 } & \\text{for } -1 \\leq x \\leq 1 \\\\\n",
    "1 & \\text{for } x > 1\n",
    "\\end{cases}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. (Coding) Plot the PDF and CDF of $X$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pdf(x):\n",
    "    #### YOUR SOLUTION HERE\n",
    "\n",
    "def cdf(x):\n",
    "    #### YOUR SOLUTION HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = np.linspace(-1.5, 1.5, 1000)\n",
    "\n",
    "pdf_values = [pdf(x) for x in xs]\n",
    "cdf_values = [cdf(x) for x in xs]\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(xs, pdf_values, label='PDF')\n",
    "plt.title('Probability Density Function (PDF)')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('f(x)')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(xs, cdf_values, label='CDF')\n",
    "plt.title('Cumulative Distribution Function (CDF)')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('F(x)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1.3: Morgan's Law\n",
    "Let $E_1$ and $E_2$ are mutually independent events. Show that $\\bar{E}_1$ and $\\bar{E}_2$ are also mutually indepent. Remark Morgan's law: $p(\\bar{A}\\bar{B}) = p(\\overline{A + B})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### YOUR SOLUTION HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1.4: LRU Technique \n",
    "\n",
    "Consider a system that cache files using a Least Recently Used (LRU) technique. In this system, the probability of requesting a popular file is $p(P) = 0.10$. If a cache's update happens, the probability of requesting a popular file is $p(P|U) = 0.20$. However, if no update has happened, the probability is only $p(P | \\bar{U}) = 0.05$. Find:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. The probability of an update $p(U)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### YOUR SOLUTION HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. The probability of an update given a popular file was requested $p(U|P)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### YOUR SOLUTION HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1.5: Independence\n",
    "\n",
    "Due to variations in the distance from Earth, the arrival time of daily updates of a robot on Mars follows a uniform distribution between 7 AM and 8 AM. Based on the events $A = \\text{update has not arrived by 7:30 AM}$ and $B = \\text{update has arrived by 7:31 AM}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Are A and B independent? Prove your answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### YOUR SOLUTION HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Find:\n",
    "$p(B|A)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### YOUR SOLUTION HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Find:\n",
    "$p(A|B)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### YOUR SOLUTION HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1.6: Probability Density Function\n",
    "\n",
    "Consider the random variable $X$ with pdf $f(x)$ given by:\n",
    "$$f(x) = \n",
    "\\begin{cases}\n",
    "a(1+x), & -1 < x \\leq 0 \\\\\n",
    "a(1-x), & 0 < x \\leq 1 \\\\\n",
    "0, & \\text{elsewhere}\n",
    "\\end{cases}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a. Find the constant $a$ and plot $f(x)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### YOUR SOLUTION HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### YOUR SOLUTION HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. Define the CDF $F(x)$ and plot it.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### YOUR SOLUTION HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### YOUR SOLUTION HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c. Find point $b$ such that:\n",
    "$p(X > b) = \\frac{1}{2}p(X \\leq b)$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### YOUR SOLUTION HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 7: Joint Probability\n",
    "About joint probabilities, prove $\\int_{-\\infty}^{\\infty} \\int_{-\\infty}^{\\infty} p(x,y) \\, dxdy = 1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### YOUR SOLUTION HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Distribution Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2.1: Expectation & Variance\n",
    "\n",
    "We consider the coffee consumption of two Computer Science students, Alice and Bob.\n",
    "Let $X$ and $Y$ be random variables characterizing the number of coffees that Alice and Bob respectively drink in a day.\n",
    "These are **discrete** random variables.\n",
    "The possible numbers of daily coffees for each programmer are denoted as $\\mathcal{X}$ for Alice and $\\mathcal{Y}$ for Bob.\n",
    "We denote their probability mass functions as $p_X$ and $p_Y$.\n",
    "\n",
    "We model Alice's consumption as follows:\n",
    "- $\\mathcal{X} = \\{0, 1, 2, 3, 4, 5\\}$\n",
    "- $p_X = [0.05, 0.10, 0.40, 0.25, 0.10, 0.10]$\n",
    "\n",
    "where $p_{X}[x] = p_{X}(x), \\quad x \\in \\mathcal{X}$.\n",
    "\n",
    "We model Bob's consumption as follows:\n",
    "- $\\mathcal{Y} = \\{0, 1, 2, 3, 4, 5, 6, 7\\}$\n",
    "- $p_Y = [0.00, 0.01, 0.09, 0.20, 0.30, 0.20, 0.10, 0.10]$\n",
    "\n",
    "where $p_{Y}[y] = p_{Y}(y), \\quad y \\in \\mathcal{Y}$.\n",
    "\n",
    "For instance, the probability that Bob drinks 2 coffees today is $p_{Y}(2) = 0.09$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2.1a) Theoretical Questions\n",
    "#### 1. Give the definition of the expected value of $X$, denoted as $\\mathbb{E}[X]$.\n",
    "#### 2. Show that the expectation is linear, e.g. by showing that from $\\mathbb{E}[aX + bY] = a \\mathbb{E}[X] + b \\mathbb{E}[Y]$ for constants $a$ and $b$.\n",
    "#### 3. Give the definition of the variance of $X$, denoted as $\\mathbb{V}[X]$.\n",
    "#### 4. Show that $\\mathbb{V}[cX] = c^2 \\mathbb{V}[X]$ for any constant $c \\in \\mathbb{R}$.\n",
    "#### 5. Give the formula connecting the variance $\\mathbb{V}[X]$ and the standard deviation $\\sigma_X$ of $X$.\n",
    "#### 6. Considering $\\mathcal{X}$ and $p_X$:\n",
    "    1. Compute the expected value of $X$.\n",
    "    2. Compute the variance of $X$.\n",
    "    3. Compute the standard deviation of $X$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   \n",
    "#### YOUR SOLUTION HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2.1b) Practical Questions\n",
    "We instantiate the set of possible values for $X$ and $Y$, and their associated possibilities below using numpy arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_values = np.array([0, 1, 2, 3, 4, 5])\n",
    "x_probabilities = np.array([0.05, 0.10, 0.40, 0.25, 0.10, 0.10]) # NOTE: x_probabilities[i] == probability of x_values[i]\n",
    "y_values = np.array([0, 1, 2, 3, 4, 5, 6, 7])\n",
    "y_probabilities = np.array([0.00, 0.01, 0.09, 0.20, 0.30, 0.20, 0.10, 0.10])\n",
    "\n",
    "# Sanity check, the probabilities must sum to 1 (taking into account numerical precision)\n",
    "assert np.isclose(x_probabilities.sum(), 1), \"Probabilities should sum to 1.\"\n",
    "assert np.isclose(y_probabilities.sum(), 1), \"Probabilities should sum to 1.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us visualize their PMF below.\n",
    "Based on this visualization, think about which random variable has the highest variance and think about what should roughly be the expected value of each random variable. Check if it is coherent with what you computed for $X$.\n",
    "(This is not a question, just a first sanity check.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating figure with 1 row and 2 columns\n",
    "fig, axs = plt.subplots(1, 2, sharex=True, sharey=True, figsize=(12, 4), squeeze=True)\n",
    "\n",
    "# x\n",
    "axs[0].bar(x_values, x_probabilities)\n",
    "axs[0].set_ylabel(\"p_X(x)\")\n",
    "axs[0].set_xlabel(\"x\")\n",
    "\n",
    "# y\n",
    "axs[1].bar(y_values, y_probabilities)\n",
    "axs[1].set_ylabel(\"p_Y(y)\")\n",
    "axs[1].set_xlabel(\"y\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Implement the functions `expectation`, `variance` and `standard_deviation` below to respectively compute the expected value, the variance and the standard deviation using the formula you provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expectation(possible_values: np.ndarray, probabilities: np.ndarray):\n",
    "    #### YOUR SOLUTION HERE\n",
    "\n",
    "def variance(possible_values: np.ndarray, probabilities: np.ndarray):\n",
    "    #### YOUR SOLUTION HERE\n",
    "\n",
    "def standard_deviation(possible_values: np.ndarray, probabilities: np.ndarray):\n",
    "    #### YOUR SOLUTION HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  2.   Verify the implementation of the above functions:\n",
    "   \n",
    "   a. Does the value of $\\mathbb{E}[X]$ you computed previously match the output of your implemented function?\n",
    "   \n",
    "   b. Do you find that $\\mathbb{E}[Y]$ is roughly what you could have guessed by visualizing the PMF of $Y$?\n",
    "   \n",
    "   c. Do you find that $\\mathbb{V}[cX] = c^2 \\mathbb{V}[X]$ for any value of $c \\in \\mathbb{R}$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### YOUR SOLUTION HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### YOUR SOLUTION HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  3. Interpret the results:\n",
    "   \n",
    "a. Who drinks the most coffee in a day in expectation?\n",
    "\n",
    "b. Who tends to have the most variable number of daily coffees?\n",
    "\n",
    "c. How many coffees in total do Alice and Bob drink daily in expectation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### YOUR SOLUTION HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2.2: Kurtosis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A PhD student from IAS is attempting to log into a PC in the robotic lab.\n",
    "The problem is that this PhD student is used to using an AZERTY keyboard layout, while all PCs in the lab either use a QWERTY or QWERTZ keyboard layout on the login screen.\n",
    "We would like to study the number of trials this PhD student needs to successfully log in.\n",
    "We denote as $X$, $Y$, and $Z$ the random variables characterizing the number of attempts it takes for the student to successfully log in when using an AZERTY, a QWERTY, and a QWERTZ layout respectively.\n",
    "We model these distributions using independent geometric distribution.\n",
    "\n",
    "The geometric distribution models the occurrence of the first success in an endless series of independent and identically distributed Bernoulli trials.\n",
    "A Bernoulli trial, also known as a binomial trial, is a random experiment that results in one of two outcomes: \"success\" or \"failure,\" with the probability of success represented as $p_{success}$.\n",
    "The probability mass function for a geometric distribution is defined as follows:\n",
    "$$\n",
    "p(k) = (1 - p_{success})^{k-1} p_{success}\n",
    "$$\n",
    "and it denotes the probability of being successful on the $k$-th trial.\n",
    "\n",
    "In the described setting, we consider different probabilities of success for each keyboard layout:\n",
    "- AZERTY, where the success of an attempt is highly probable: $p_{succcess}^{X} = 0.9$,\n",
    "- QWERTY, where the success of an attempt is somewhat probable: $p_{succcess}^{Y} = 0.5$,\n",
    "- QWERTZ, where the success of an attempt is quite unlikely: $p_{succcess}^{Z} = 0.2$.\n",
    "\n",
    "We then note $p_X(k)$ as the probability that the PhD student successfully logs in on its exactly $k$-th attempt and use analogous notations for $p_Y$ and $p_Z$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2.2a) Theoretical Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Give the definition of the kurtosis of a random variable $X$, denoted as $\\mathrm{Kurt}[X]$. You can use $\\mu$ and $\\sigma$ as symbols to denote the expected value and standard deviation of $X$ respectively.\n",
    "\n",
    "#### 2. Explain in one sentence what the kurtosis measures about a probabilistic distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### YOUR SOLUTION HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2.2b) Practical Questions\n",
    "\n",
    "We will rely on the `scipy` package to use the geometric distribution more easily.\n",
    "We start by instantiating a random variable for each keyboard layout and storing them in a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define probability of success at each trial for each keyboard layout\n",
    "ps_success = {\n",
    "    \"azerty\": 0.9,\n",
    "    \"qwerty\": 0.5,\n",
    "    \"qwertz\": 0.2,\n",
    "}\n",
    "\n",
    "# Random variables for each keyboard layout\n",
    "geoms = {layout: geom(p_success) for layout, p_success in ps_success.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us visualize the PMF for each keyboard layout. We plot in linear and logarithmic scale to see *how big* the tail of each distribution is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute pmf up to some maximum number of trials\n",
    "# We assume that higher values have too low probability to be\n",
    "# significant.\n",
    "max_num_trials = 15\n",
    "num_trials = np.arange(0, max_num_trials+1)\n",
    "pmfs = {layout: g.pmf(num_trials) for layout, g in geoms.items()}\n",
    "\n",
    "# Plot pmfs\n",
    "fig, axs = plt.subplots(1, 2, figsize=(12, 4), sharex=True)\n",
    "for layout, g in geoms.items():\n",
    "    # linscale\n",
    "    axs[0].plot(num_trials, pmfs[layout], \"o\", label=layout)\n",
    "    axs[0].vlines(num_trials, 0, pmfs[layout])\n",
    "    # logscale\n",
    "    axs[1].plot(num_trials, pmfs[layout], \"o\", label=layout)\n",
    "    axs[1].set_yscale('log')\n",
    "axs[0].set_xlabel(\"Number of trials\")\n",
    "axs[1].set_xlabel(\"Number of trials\")\n",
    "axs[0].set_ylabel(\"Probability of success after exactly n trials\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should notice that these distributions have different sizes of tail and it is where the kurtosis comes in handy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Implement the function `kurtosis` below to compute the skewness using the formula you provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kurtosis(possible_values: np.ndarray, probabilities: np.ndarray):\n",
    "    #### YOUR SOLUTION HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Compare the kurtosis values your function outputs against the kurtosis values computed by `scipy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### YOUR SOLUTION HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Could you have anticipated that the distribution for certain keyboard layouts have higher/lower kurtosis? Explain.\n",
    "#### 4. In this particular context, give a semantic interpretation to the size of the tail of the distribution.\n",
    "#### 5. Do you notice how our way of computing the kurtosis underestimates the kurtosis of long-tailed distributions? Provide an explaination by thinking about possible wrong assumptions we have made."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### YOUR SOLUTION HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2.3: Median vs. Expectation\n",
    "\n",
    "Your favorite hard-drive manufacturer is happy to advertise that, in expectation, their hard drives break after only 100,000 hours of usage.\n",
    "Despite your trust in this company, you decide to investigate a little bit.\n",
    "\n",
    "You decide to assume that the time to failure/breakage follows an exponential distribution.\n",
    "Let $T$ be a **continuous** random variable following an exponential distribution, i.e. its probability density function (PDF) is defined as\n",
    "$$\n",
    "p_T(t) = \\lambda e^{-\\lambda t}, \\quad \\lambda > 0, \\quad t \\in [0, +\\infty).\n",
    "$$\n",
    "We consider $T$ to characterize the number of years it takes for a hard drive to break."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2.3a) Theoretical Questions\n",
    "\n",
    "#### 1. Give a closed form for the expected value of $T$, denoted as $\\mathbb{E}[T]$.\n",
    "#### 2. Compute the value of $\\lambda$ that would yield an average lifetime of 100,000 hours, assuming $\\lambda$ is in breakages/second.\n",
    "#### 3. Give the cumulative density function (CDF) of $T$, denoted as $F_T(t)$, as a function of $p_t$.\n",
    "#### 4. Give closed-form for $F_T(t)$.\n",
    "#### 5. Give the definition of the median of a random variable.\n",
    "#### 6. Compute the median of $T$, denoted $\\mathrm{med}_T$ as a function of $\\lambda$, and give a numerical value using the value of $\\lambda$ you computed before.\n",
    "#### 7. How does the lifetime of 50\\% of the manufacturer's hard drive compare to the advertised expected lifetime?\n",
    "#### 8. Propose one or more possible issue(s) with our reasoning which might have led to unfair results to your favorite manufacturer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### YOUR SOLUTION HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2.3b) Practical Questions\n",
    "Let us compute the median using Python.\n",
    "Firstly, we visualize the PMF.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lmbda = 1e-5\n",
    "max_t = 1e6\n",
    "stepsize = 1000 # stepsize should be small for high lambdas\n",
    "assert lmbda > 0, \"The value of lambda should be strictly positive!\"\n",
    "\n",
    "# Discretizing set of ws, pdf and cdf\n",
    "t = np.linspace(0, max_t, num=int(max_t / stepsize))\n",
    "p_T = lmbda * np.exp(-lmbda * t)\n",
    "F_T = np.cumsum(p_T) * stepsize # emulating integration as sum of small rectangles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(12, 4), sharex=True)\n",
    "axs[0].plot(t, p_T)\n",
    "axs[0].set_xlabel(\"time to failure t [seconds]\")\n",
    "axs[0].set_ylabel(\"p_T(t)\")\n",
    "axs[1].plot(t, F_T)\n",
    "axs[1].set_ylabel(\"F_T(t)\")\n",
    "axs[1].set_xlabel(\"time to failure t [seconds]\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Implement the function `get_theoretical_median` from the formula you derived before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_med_theoretical():\n",
    "    #### YOUR SOLUTION HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Compute the median by leveraging `F_T` that we computed before in the function `get_med_practical`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_med_practical():\n",
    "    #### YOUR SOLUTION HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Public Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "med_theoretical = get_med_theoretical()\n",
    "med_practical = get_med_practical()\n",
    "print(f\"med_theo = {med_theoretical:.0f}, med_prac = {med_practical}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. If there is a discrepency between the theoretical and the practical median, how do you explain it? How could you simply fix it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### YOUR SOLUTION HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2.4: Covariance and Correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2.4a) Theoretical Questions\n",
    "1. Give the definition of the covariance $\\mathrm{Cov}[X, Y]$ between two random variables $X$ and $Y$.\n",
    "2. Show that $\\mathbb{V}[X + Y] = \\mathbb{V}[X] + \\mathbb{V}[Y] + 2 \\mathrm{Cov}[X, Y]$. What does this formula become when $X$ and  $Y$ are independent?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### YOUR SOLUTION HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Information Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1: Entropy\n",
    "You are given an array of integers $X$ with $\\forall x \\in X: x \\geq 0$. Our goal is the calculate the entropy of $X$. Therefore follow the steps below:\n",
    "\n",
    "### 1.  Implement the function ```get_pmf_1d(X: np.array)``` which outputs the probability mass function for X, so p[x]=relative frequency of x in X. Think about for which indices $x$ p[x] must store a value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pmf_1d(X: np.array):\n",
    "    #### YOUR SOLUTION HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Public Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_cases = [(np.array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]),np.ones(10) * 0.1),(np.array([0, 2, 2, 5, 8, 1, 6, 0, 8, 9]),np.array([0.2, 0.1, 0.2, 0, 0, 0.1, 0.1, 0, 0.2, 0.1]))]\n",
    "            \n",
    "\n",
    "for i, (X, sol) in enumerate(test_cases):\n",
    "    # Compute DGL step using your function\n",
    "    pred = get_pmf_1d(X)\n",
    "    \n",
    "    test_almost_equal_array(f'{i}', pred, sol)\n",
    "    test_almost_equal(f'Sum {i}', np.sum(pred), 1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Implement the function ```compute_entropy(p: np.array)``` which computes the entropy $H(X)$ of the probability mass function $p$ using get_mf_1d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_entropy(X: np.array):\n",
    "    #### YOUR SOLUTION HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Public Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_cases = [\n",
    "    ((np.array([1, 2, 3, 4])), 2),\n",
    "    ((np.array([0, 0, 0, 0])), 0),\n",
    "]\n",
    "\n",
    "for i, (X, sol) in enumerate(test_cases):\n",
    "    # Compute DGL step using your function\n",
    "    pred = compute_entropy(X)\n",
    "    test_almost_equal(f'{i}', pred, sol)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2: Compute for a Binary erasure channel the Channel Capacity (Theory question)\n",
    "In this question compute the channel capacity of a binary erasure channel with erasure probability $p_e = 0.25$ and put your result as latex code in the underlying markdown cell. Furthermore, what are the probabilities of the input $P(x)$ for which the channel capacity is achieved?\n",
    "\n",
    "The channel capacity is given by: \n",
    "\n",
    "$$C=\\max _{p(x)} \\mathrm{I}(X ; Y)$$\n",
    "\n",
    "*Hint:* Think about the probabilities of the input P(x). How are the input probabilities related to the max operator?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### YOUR SOLUTION HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3: Mutual Information\n",
    "Now you are given two arrays of integers $X$ and $Y$, where all elements are greater than or equal to 0 (cf. Question 1). Compute the mutual information $I(X;Y)$ between the two arrays.\n",
    "\n",
    "\n",
    "### 1. Implement the function ```get_pmf_2d(X: np.array, Y: np.array)``` which outputs the probability mass function for the joint probability of X and Y, so p[x, y]=relative frequency of x and y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pmf_2d(X: np.array, Y: np.array):\n",
    "    #### YOUR SOLUTION HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Public Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_cases = [((np.array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]),np.array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])),np.eye(10,10) * 0.1),((np.array([0, 1, 2, 3]),np.array([3, 2, 2,1 ])),np.array([[0, 0, 0, 0.25], [0, 0, 0.25, 0], [0, 0, 0.25, 0], [0, 0.25, 0, 0]]))]\n",
    "            \n",
    "\n",
    "for i, (X, sol) in enumerate(test_cases):\n",
    "    # Compute DGL step using your function\n",
    "    pred = get_pmf_2d(*X)\n",
    "    test_almost_equal_array(f'{i}', pred, sol)\n",
    "    test_almost_equal(f'Sum {i}', np.sum(pred), 1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Implement the function ```compute_mutual_information(X: np.array, Y: np.array)``` which computes the mutual information $I(X;Y)$ between the two arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mutual_information(X: np.array, Y: np.array):\n",
    "    #### YOUR SOLUTION HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Public Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_cases = [\n",
    "    ((np.array([1, 2, 3, 4]), np.array([0, 1, 2, 3])), 1.5),\n",
    "    ((np.array([1, 1, 2, 1]), np.array([1, 1, 1, 2])), -0.08496),\n",
    "]\n",
    "\n",
    "# X = np.random.choice(4, 10000, p=[0.25, 0.25, 0.25, 0.25])\n",
    "\n",
    "\n",
    "for i, ((X, Y), sol) in enumerate(test_cases):\n",
    "    # Compute DGL step using your function\n",
    "    pred = compute_mutual_information(X, Y)\n",
    "    test_almost_equal(f'{i}', pred, sol)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4: Average Conditional Entropy of an Image\n",
    "\n",
    "Images can be represented in pixel matrices $M \\in \\mathcal{R}^{h\\times w \\times c}$, where $h$ is the height and $w$ is the width of the image, and $c$ is the channel dimension.\n",
    "In black-and-white images, $M[m,n]$ stores the pixel color of the $m$-th row and $n$-th column of the image, which is 1 for white and 0 for black.\n",
    "In RGB arrays you will instead have a 3D value for each pixel, with values between 0 and 255, but in this exercise, we focus on black-and-white images for simplicity.\n",
    "Storing such images can take up lots of storage, especially for high-resolution pictures.\n",
    "Instead, many image storage formats, such as JPEG, use a lossy compression of the images.\n",
    "The idea we consider here is that if we know the neighboring pixels of a pixel at position $(m,n)$, this information can allow us to predict what is the pixel value at $(m,n)$, e.g., using maximum likelihood.\n",
    "How good this approximation is can be estimated with the *average conditional entropy* of this relationship, which we will compute in this exercise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Complete the function `show_image` which is given a 2D pixel matrix of a black-and-white image as described before, and displays the image in a matplotlib plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_image(img: np.array, title: str=''):\n",
    "    #### YOUR SOLUTION HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Public Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "height, width = 100, 100 \n",
    "bw_image = np.random.choice([0, 1], size=(height, width)) #, p=[0.01, 0.99])\n",
    "show_image(bw_image, title='Random BW Image')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. It will help us to define a 'code' for the neighbors of a pixel, which we do as follows:\n",
    "Looking at a pixel, we start with the upper left neighbor and go around the image clockwise to get the remaining neighbors (s. Example).\n",
    "The sequence of 0's and 1's that we encounter can be viewed as a binary number, which we can transform into an integer to obtain a decimal code of the neighbors.\n",
    "Compute the 'neighbor code' for a pixel in the function ```get_neighbor_code```.\n",
    "\n",
    "*Note:* Make sure that your function checks that the pixel index is not at the borders of the image (because not all neighbors are available), and if not returns the neighbor code -1.\n",
    "In practice, one could define a padding for such pixels, but we will only consider pixels that are properly inside the image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example:** In the plot above, the sequence is '10101110', which yields the neighbor code 174."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_neighbor_code(pixels: np.array, m: int, n: int):\n",
    "    #### YOUR SOLUTION HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Public Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_cases = [\n",
    "    ((np.array([\n",
    "        [1, 0, 1], \n",
    "        [0, -1, 0], \n",
    "        [1, 1, 1]\n",
    "    ]), 1, 1), 174),\n",
    "    ((np.array([\n",
    "        [1, 0, 1], \n",
    "        [0, -1, 0], \n",
    "        [1, 1, 1]\n",
    "    ]), 0, 1), -1),\n",
    "]\n",
    "\n",
    "# X = np.random.choice(4, 10000, p=[0.25, 0.25, 0.25, 0.25])\n",
    "\n",
    "\n",
    "for i, ((X, m, n), sol) in enumerate(test_cases):\n",
    "    # Compute DGL step using your function\n",
    "    pred = get_neighbor_code(X, m, n)\n",
    "    test_almost_equal(f'{i}', pred, sol)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Now implement two functions:\n",
    "\n",
    "i. ```compute_px```computes a 1D probability distribution where p[x]=probability of seeing x in the whole image array, with $x\\in\\{0,1\\}$. \n",
    "\n",
    "ii. ```compute_pxy```computes the joint probability of the pixels and their neighbors. Here we will use the neighbor code from 4b) to translate the neighbors of a pixel into a proper index for our array.  Therefore p[x,y] will hold the joint probability of seeing the middle pixel $x$ and the neighbor code $y$.\n",
    "\n",
    "*Note:* As described, we only consider pixels that are not at the border, so only consider these pixels for your computations.\n",
    "\n",
    "*Note:* Both distributions should have the proper shape. Think about what this shape must be for p[x,y] (think about the range of y values possible).\n",
    "\n",
    "*Hint:* The functions ```get_pmf_1d``` and ```get_pmf_2d```may help you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_px(pixels: np.array):\n",
    "    #### YOUR SOLUTION HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_pxy(pixels: np.array):\n",
    "    #### YOUR SOLUTION HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Public Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_cases = []\n",
    "i1 = np.array([\n",
    "        [1, 0, 1], \n",
    "        [0, 1, 0], \n",
    "        [1, 1, 1]\n",
    "])\n",
    "p_x1 = np.array([0, 1])\n",
    "p_xy1 = np.zeros((2, 256))\n",
    "p_xy1[1, 174] = 1\n",
    "\n",
    "test_cases.append(\n",
    "    (i1, (p_x1, p_xy1))\n",
    ")\n",
    "\n",
    "\n",
    "for i, (X, (s1, s2)) in enumerate(test_cases):\n",
    "    # Compute DGL step using your function\n",
    "    pred1 = compute_px(X)\n",
    "    pred2 = compute_pxy(X)\n",
    "    test_almost_equal(f'{i}', pred, sol)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Using the defined functions, now compute the average conditional entropy of the pixels and their neighbors. Implement your solution in the function ```compute_average_conditional_entropy```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cond_entropy(pixels: np.array):\n",
    "    #### YOUR SOLUTION HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Public Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "height, width = 100, 100  # Example size, can be adjusted\n",
    "# Generate a random black-and-white image (binary values 0 and 1)\n",
    "bw_image = np.random.choice([0, 1], size=(height, width))\n",
    "test_cases = [\n",
    "    (bw_image, 7.96487)\n",
    "]\n",
    "\n",
    "for i, (X, sol) in enumerate(test_cases):\n",
    "    # Compute DGL step using your function\n",
    "    pred = compute_cond_entropy(X)\n",
    "    test_almost_equal(f'{i}', pred, sol)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5: Compute the KL divergence between these two distributions in both directions.\n",
    "\n",
    "In this task, we want to compute the KL divergence between two distributions. To compute the KL divergence we first need 2 distributions that we wanna compare. In our case, we take the following 2 distributions\n",
    "\n",
    "$\n",
    "X_1 \\sim \\mathcal{N}(0, 1)\n",
    "$\n",
    "\n",
    "$\n",
    "X_2 \\sim \\mathcal{N}(5, 6)\n",
    "$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. PDF of a normal distribution\n",
    "\n",
    "First, we want to be able to compute the probability of a value $x$ for the given 2 distributions. Therefore implement the function ```normal_pdf(x: float, mean: float, std: float)``` which computes the probability of values $x$ (array of values) for a normal distribution with mean $mean$ and standard deviation $std$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normal_pdf(x:np.ndarray, mu:float, sigma:float):\n",
    "    #### YOUR SOLUTION HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Public Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_cases = [(np.array([0.0,-1.0,1.0,1.5,-1.5]), np.array([0.39894228, 0.24197072, 0.24197072, 0.1295176,  0.1295176])),(np.array([0.0,-1.0,1.0,1.5,-1.5]),[0.12447389, 0.11602242, 0.13147063, 0.13432575, 0.11135996])]\n",
    "\n",
    "mu = [0,4]\n",
    "sigma = [1,8]\n",
    "y = normal_pdf(np.array([0.0,-1.0,1.0,1.5,-1.5]), 4, 8)\n",
    "for i, (X, sol) in enumerate(test_cases):\n",
    "    # Compute DGL step using your function\n",
    "    pred = normal_pdf(X,mu[i],sigma[i])\n",
    "    test_almost_equal_array(f'{i}', pred, sol)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the function is implemented correctly, you will see a plot containting both normal distributions below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x  = np.linspace(-30, 30, 10000)\n",
    "\n",
    "y1 = normal_pdf(x, 0, 1)\n",
    "y2 = normal_pdf(x, 5, 6)\n",
    "\n",
    "plt.plot(x, y1, label='mu=0, sigma=1')\n",
    "plt.plot(x, y2, label='mu=5, sigma=6')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Implement the function ```kl_divergence(mean1: float, sigma1: float, mean2: float, sigma2: float)``` which computes the KL divergence between the two normal distributions with the given parameters.\n",
    "The general formula for the KL divergence is $D_{\\mathrm{KL}}[p: q]=\\int p \\log \\frac{p}{q} \\mathrm{~d} \\mu$ where $p$ and $q$ are the two distributions we want to compare.\n",
    "In our case, we have 2 normal distributions which has the advantage that a closed-form solution for the KL divergence exists. The KL divergence between two normal distributions is given by:\n",
    "$D_{\\text{KL}}(p \\,||\\, q) = \\log\\left(\\frac{\\sigma_q}{\\sigma_p}\\right) + \\frac{\\sigma_p^2 + (\\mu_p - \\mu_q)^2}{2 \\sigma_q^2} - \\frac{1}{2}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kl_divergence(mu1,sigma1,mu2,sigma2):\n",
    "    #### YOUR SOLUTION HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Public Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_cases = [((0,1,0,1), 0.0),((9,2,3,8),2.568147180559945),((12,4,0,1),72.80685281944005)]\n",
    "\n",
    "\n",
    "for i, (X, sol) in enumerate(test_cases):\n",
    "    # Compute DGL step using your function\n",
    "    pred = kl_divergence(*X)\n",
    "    test_almost_equal(f'{i}', pred, sol)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Now compute the KL divergence for the two given normal distributions.\n",
    "\n",
    "Hereby compute the KL-divergence once where x1 corresponds to p and x2 corresponds to q and once where x1 corresponds to q and x2 corresponds to p. Save the results in the variables kl1 and kl2 and print them. What do you observe and why? Answer this question in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kl1 = kl_divergence(0,1,5,6)\n",
    "kl2 = kl_divergence(5,6,0,1)\n",
    "\n",
    "print(f\"KL-divergence between X1 and X2: {kl1}\")\n",
    "print(f\"KL-divergence between X2 and X1: {kl2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### YOUR SOLUTION HERE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
